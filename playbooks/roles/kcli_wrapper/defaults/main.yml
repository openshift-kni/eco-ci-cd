---
# defaults file for vhub-cluster
#######################################################
# If you want that the role try to install KCLI
# dependencies (Fedora, RHEL or CentOS should be
# supported by now), set to true the following
# variable:
#######################################################
# kcli_wrp_install_depencencies: false
#######################################################
# Comment out these lines to autogenerate a pair of
# SSH both public and private keys to be used during
# cluster deployment.
#######################################################
# kcli_wrp_ssh_key:
#   filename: ~/.ssh/id_rsa_for_vhub_cluster
#   size: 2048
#######################################################
# Set the default libvirt pool to use.
# Clean mode:
# if 'also_default' is false, the default pool won't
# be removed upon running with '--tags rollback'
# if 'remove_files' is false, files that belongs to a
# pool won't be removed upon running with
# '--tags rollback'
#######################################################
kcli_wrp_libvirt:
  pool:
    name: default
    path: /var/lib/libvirt/images
    clean_mode:
      also_default: false
      remove_files: false
#######################################################
# Use the below parameters as a example.
# The comprehensive list of values can be found here:
# https://kcli.readthedocs.io/en/latest/
# KCLI Slack channel:
# https://kubernetes.slack.com/archives/CU76B52JE
#######################################################
# kcli_wrp:
#   networks:
#     - name: vhub-net
#       type: network
#       domain: example.lab
#       cidr: 172.16.33.0/24
#       secondary_cidr: fc00:52:0:1305::0/64
#       gateway: 172.16.33.1
#     - name: baremetal
#       bridge: true
#       bridgename: baremetal # <-- same to 'name' if not defined
#       nic: eth0
#       bridge_cidr_ipv4: "192.168.80.1/25"
#     - name: vhub-macvtap-net
#       macvtap: true
#       nic: eth0
#   clusters:
#   # For example, if you want to install an openshift cluster
#   # then use only parameters that make sense for the below command:
#   # kcli create cluster openshift --paramfile=parameters.yml
#   - type: openshift
#     # kcli create cluster openshift --force --paramfile=parameters.yml
#     force_installation: false
#     parameters:
#         cluster: vhub-ocp
#         version: stable
#         tag: 4.14
#         domain: example.lab
#         pool: "{{ kcli_wrp_libvirt.pool.name }}"
#         nets:
#           - vhub-net
#           - vhub-br-net
#           - vhub-macvtap-net
#         keys:
#           - "{{ kcli_wrp_ssh_key.filename }}.pub"
#         # api_ip: 172.16.33.5
#         # ingress_ip: 172.16.33.6
#         ctlplanes: 3
#         workers: 0
#         memory: 16384
#         numcpus: 4
#         disk_size: 30
#         # IMPORTANT:
#         # If you decided to set you pull_secret file
#         # as base64 string using 'base64_pull_secret'
#         # parameter, it will take precedent over
#         # 'pull_secret' paramater.
#         # Note that 'base64_pull_secret' parameter is
#         # not part of KCLI
#         base64_pull_secret: null
#         pull_secret: ~/.docker/config.json
#         apps:
#           - local-storage-operator
#           - openshift-gitops-operator
#           - advanced-cluster-management
#           - topology-aware-lifecycle-manager
#######################################################
# Comment out these lines to install both 'oc' and
# 'kubectl' tools if they are not already installed
#######################################################
# kcli_wrp_oc:
#   url: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz
#   dest: /usr/bin
#######################################################
# When defined, it generates firewalld zone files
# for the firellawd service running in the host machine
#######################################################
# kcli_wrp_firewalld:
#    zone_files:
#      - path: /etc/firewalld/zones/public.xml
#        content: |
#          <?xml version="1.0" encoding="utf-8"?>
#          <zone>
#            <short>Public</short>
#            <description>For use in public areas...</description>
#            <service name="ssh"/>
#            <service name="dhcpv6-client"/>
#            <service name="cockpit"/>
#            <service name="dhcp"/>
#            <service name="dns"/>
#            <port port="3128" protocol="tcp"/>
#            <masquerade/>
#            <forward/>
#          </zone>
#######################################################
# When defined, it generates dnsmasq drop-in file
# to be used along with the default dnsmasq instance
# running in the host machine
# [https://docs.fedoraproject.org/en-US/fedora-server/administration/dnsmasq/]
#######################################################
# kcli_wrp_dnsmasq:
#   use_nm_plugin: true
#   drop_in_files:
#     - path: /etc/NetworkManager/dnsmasq.d/70-{{ kcli_wrp.parameters.cluster }}.conf
#       content: |
#         # /etc/NetworkManager/dnsmasq.d/70-{{ kcli_wrp.parameters.cluster }}.conf
#         #
#         # This file directs dnsmasq to forward any request to resolve
#         # names under the .custom-dns.lab domain to 172.16.55.1, a
#         # custom DNS server.
#         server=/custom-dns.lab/172.16.55.1
#     - path: /etc/NetworkManager/dnsmasq.d/70-{{ kcli_wrp.parameters.cluster }}.conf
#       content: |
#         # /etc/NetworkManager/dnsmasq.d/70-{{ kcli_wrp.parameters.cluster }}.conf
#         #
#         # This file sets up the local lablab domain and
#         # defines some aliases and a wildcard.
#         local=/example.lab/
#         # The below defines a Wildcard DNS Entry.
#         address=/.example.lab/192.168.44.44
#         # Below I define some host names.  I also pull in
#         address=/openshift.example.lab/192.168.44.120
#         address=/openshift-int.example.lab/192.168.44.120
#######################################################
# When defined, it generates a systemd service that
# start off a local SSH connection against the host
# machine and bind the defined listen port to be used
# as SOCKS5 proxy. It also genarates a kubeconfig
# file along with proxy-url: socks5://<your_endpoint>
# to be used from remote systems that need to connect
# to virtualised Hub cluster running in the host
# machine. Note that the DNS resolutions will be done
# in the remote host so that, you can get advantage
# of it to define private domains.
#######################################################
# kcli_wrp_socks5_proxy:
#   description: SOCKS5 Proxy Server for...
#   username: <your_bastion_username>
#   host: <your_hypervisor_bastion_host>
#   listen_port: <socks5 listen port>
#   ssh_options: "-4"
#######################################################
# When defined, it generates HA Proxy drop-in files
# to be used in the host machine
#######################################################
# kcli_wrp_haproxy:
#   drop_in_files:
#     - path: /etc/haproxy/conf.d/70-<your_clustername>.ha-proxy.cfg
#       content: |
#         listen api-server-6443
#           log global
#           timeout connect 5s
#           timeout client 50s
#           timeout server 50s
#
#           bind :::6443
#           mode tcp
#           balance source
#           server <your_clustername>-api <your_api_server_endpoint:6443 check inter 1s
#
#         listen ingress-443
#           log global
#           timeout connect 5s
#           timeout client 50s
#           timeout server 50s
#
#           bind :::443
#           mode tcp
#           balance source
#           server <your_clustername>-ingress <your_ingress_endpoint:443 check inter 1s
#
#         listen httpd-9000
#             bind :::9000 v6only
#             mode tcp
#             balance source
#             server localhost 127.0.0.1:9000 check inter 1s
#
#         listen gitserver-3000
#             bind :::3000 v6only
#             mode tcp
#             balance source
#             server localhost 127.0.0.1:3000 check inter 1s
#######################################################
# NOTE: Do not change these default values
# KCLI default credentials are the below ones.
# These files are read after cluster deployment is
# done and store as ansible_facts into the following
# variables to be used by next playbooks:
#
# kcli_wrp_credentials:
#   <cluster_name_0>:
#     kubeconfig: ...
#     b64_kubeconfig: ...
#     kubeadmin_password: ...
#   ...
#   <cluster_name_N>:
#     kubeconfig: ...
#     b64_kubeconfig: ...
#     kubeadmin_password: ...
#
# Uncomment (or define again) these lines below to
# get those variables be loaded after cluster
# installation is done.
# In any case, the credentials will still be available
# in the shown host node path below.
#######################################################
# kcli_wrp_credentials:
#   clusters_details: ~/.kcli/clusters
#   kubeconfig: auth/kubeconfig
#   kubeadmin_password: auth/kubeadmin-password
